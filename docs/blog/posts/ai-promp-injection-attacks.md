---
title: "AI Prompt Injection Attacks"
date: 2023-07-31
authors:
  - grobauskas
categories:
  - AI
  - Security
---

Bruce Schneier has hit the nail on the head in his recent post on AI prompt injection attacks. Schneier feels itâ€™s not possible to fully secure large language models (LLMs) against this kind of attack. Essentially, you can use AI to generate injection prompts, but read the article to learn more.

[Automatically Finding Prompt Injection Attacks](https://www.schneier.com/blog/archives/2023/07/automatically-finding-prompt-injection-attacks.html)

