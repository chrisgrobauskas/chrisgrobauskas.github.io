---
layout: post
title:  "Worthy Links July 24"
date:   2023-07-24 20:00:00 -0500
categories: news
---
> “The ignorance of one voter in a democracy impairs the security of all.” ~ John F. Kennedy

Three worthy links to read:

## Security Through Obscurity
Another example of why security through obscurity does not work.  Radio systems used by police and military outside the US have vulnerabilities that have existed since the 1990s.

The system is also used within “pipelines, railways, the electric grid, mass transit, and freight trains” in many countries, including the U.S.

Do you agree that security through obscurity is not effective?

[Code Kept Secret for Years Reveals Its Flaw—a Backdoor](https://www.wired.com/story/tetra-radio-encryption-backdoor/)

## Baseboard Management Controllers
Vulnerable firmware for baseboard management controllers can allow out of band access to servers to upload and execute arbitrary code.

The severity of the vulnerability depends on several configuration settings, but this is not good.

Read more at Ars Technica:

[Firmware vulnerabilities in millions of computers could give hackers superuser status](https://arstechnica.com/security/2023/07/millions-of-servers-inside-data-centers-imperiled-by-flaws-in-ami-bmc-firmware)

## Human In the Loop

What if an AI monitored whether you followed all rules and laws that applied to you?  

This chilling and dystopian thought is the subject of a guest post by Jon Penney on Bruce Schneier’s blog (link at bottom).

A couple of thoughts:

1) The article is a good reminder of why there should be a “human in the loop” for higher risk activities.  This is not a new concept.  Consider nuclear weapon launch procedures.  Multiple people must carry out any order to launch because there are serious consequences.

Accusing someone of an illegal activity could carry serious consequences and should require a human in the loop.

2) Penney covers the legal risks to the targets of AI-based decisions, but one would hope legal risks will also exist for the owners of misbehaving AI systems.

While businesses need to carefully consider AI efforts on their technical and business merits, they also need to consider whether outcomes are fair and defensible in court.

Take, for example, an AI system that identifies potential fraud.  Is the AI that made the recommendation accurate and free of error and bias?  Can the recommendations the AI makes be explained?  Was an actual decision made by the AI, or was it only a recommendation reviewed against other information?  

Accusing someone of fraud is serious and could lead to a lawsuit or choosing not to fulfill an otherwise required action under a contract.

I am not a lawyer, but to me, a human in the loop making decisions based on a recommendation seems more defensible.  The human can review the recommendation and hopefully catch errors.

What do you think?  What systems need a human in the loop?

[AI and Micro-directives](https://www.schneier.com/blog/archives/2023/07/ai-and-microdirectives.html)

