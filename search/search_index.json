{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Chris Grobauskas' Blog","text":"<p>I am a Senior Technology Engineer at State Farm Insurance, but the perspectives you find on this site are entirely my own.</p> <p>After twenty-five years exploring technology across platforms like Unix, Linux, Mainframe, and AWS, I'm filled with a sense of wonder and optimism for what we can build.</p> <p>This blog is my way of sharing what excites me and hopefully offering a little encouragement.</p>"},{"location":"resume/","title":"Summary","text":"<p>With 25 years of experience in technology, I have worked across a range of platforms including Linux, Mainframe, and AWS. </p> <p>My background spans software development, database administration, and large-scale system modernization, with a focus on delivering practical solutions for complex enterprise challenges. </p> <p>I enjoy collaborating with cross-functional teams, mentoring others, and sharing knowledge to help drive continuous improvement. </p> <p>My approach is hands-on and approachable, aiming to make technology accessible while maintaining high standards for quality and reliability. </p>"},{"location":"resume/#work-experience","title":"Work Experience","text":""},{"location":"resume/#state-farm","title":"State Farm","text":""},{"location":"resume/#senior-technology-engineer","title":"Senior Technology Engineer","text":""},{"location":"resume/#bloomington-illinois-united-states","title":"Bloomington, Illinois, United States","text":""},{"location":"resume/#2024-2025","title":"2024 - 2025","text":"<ul> <li>Led data modernization initiatives for the largest Property and Casualty (P&amp;C) claim system in the world, driving strategic improvements across core systems.</li> <li>Collaborated with cross-functional technical teams to support and enhance production claim systems, ensuring high availability and performance.</li> <li>Consulted with enterprise technology teams, advocating for solutions aligned with business objectives.</li> <li>Influenced software engineering and data management best practices across multiple teams, fostering a culture of quality and efficiency.</li> <li>Evaluated new products and drove proof-of-concepts, advocating for innovative solutions to enhance enterprise capabilities.</li> <li>Resolved major production outages by coordinating rapid, multi-team troubleshooting efforts, minimizing downtime and business impact.</li> <li>Communicated complex technical solutions and project progress to senior leadership, ensuring transparency and alignment.</li> <li>Mentored junior engineers and cultivated a collaborative, innovative team culture.</li> </ul>"},{"location":"resume/#prior-experience","title":"Prior Experience","text":""},{"location":"resume/#state-farm_1","title":"State Farm","text":""},{"location":"resume/#lead-data-engineer","title":"Lead Data Engineer","text":""},{"location":"resume/#2022-2024","title":"2022 - 2024","text":""},{"location":"resume/#core-team-member-on-enterprise-systems-modernization-efforts","title":"Core Team Member on Enterprise Systems Modernization Efforts","text":"<ul> <li>Automated Db2 to Postgres migration: Developed a Python program to convert the DDL for 24,000 Db2 tables to Postgres. Collaborated with enterprise data enablement teams to review vendor tools for schema migration.</li> <li>Migrated billions of rows with zero downtime: Successfully replicated billions of rows of legacy data from z/OS Db2 to Aurora Postgres using Qlik Replicate, enabling early experimentation with Aurora Postgres.</li> <li>Streamlined deployment with automation: Implemented Gitlab CI pipelines and Flyway to automate DDL deployment for Aurora Postgres, boosting efficiency and consistency.</li> <li>Troubleshooting and support: Acted as the go-to data engineer for troubleshooting production issues, quickly resolving problems and minimizing downtime.</li> <li>Leadership and mentorship: Provided technical leadership and mentorship to other engineers, fostering a collaborative learning environment.</li> <li>Knowledge sharing: Presented at area, guild, and cloud summit meetings to share knowledge and best practices.</li> </ul>"},{"location":"resume/#data-analyst-database-administrator","title":"Data Analyst - Database Administrator","text":""},{"location":"resume/#2016-2022","title":"2016 - 2022","text":""},{"location":"resume/#db2-and-postgres-database-administrator-specializing-in-automation-performance-tuning-and-data-migration","title":"Db2 and Postgres Database Administrator specializing in automation, performance tuning, and data migration","text":"<ul> <li>Collaborative problem solver: Worked closely with application developers and Db2 performance teams to implement database changes, tune query performance, and resolve performance issues.</li> <li>Managed a complex data landscape: Our Db2 production environment has over 24,000 tables across 30 Db2 subsystems, including High Availability Parallel Sysplex Db2 Data Sharing Groups.</li> <li>Data migration expert: Developed and executed data migration jobs during a hardware sunset project, successfully reducing the production hardware footprint for High Availability Sysplex systems by 33% for claims data.</li> <li>Postgres: Assisted team members with database changes and debugging application issues in test and production environments.</li> </ul>"},{"location":"resume/#software-developer","title":"Software Developer","text":""},{"location":"resume/#2010-2016","title":"2010 - 2016","text":""},{"location":"resume/#team-lead-for-enterprise-claim-system-ecs-printfax-application-and-server-type-owner-for-ecs-file-share-server","title":"Team Lead for Enterprise Claim System (ECS) Print/Fax application and server type owner for ECS File Share server","text":"<ul> <li>Supported a J2EE application that managed the printing, bundling, and delivery of prints and faxes for the world's largest P&amp;C claim system.</li> <li>Developed a software solution to streamline the claims file delivery process for the legal department, improving efficiency and accuracy.</li> <li>Worked on an enterprise print consolidation effort that enabled the use of enterprise printers by the ECS application to increase security and cost efficiency.</li> </ul>"},{"location":"resume/#enterprise-claim-system-ecs-program","title":"Enterprise Claim System (ECS) Program","text":"<ul> <li>Led efforts to bring over 200 data flows to rest before conversion by driving discussions across multiple application teams to create a system-wide plan.</li> <li>Contributed to the ECS Data Conversion Project by writing data extract jobs, identifying data quality issues, and proactively cleaning up data quality issues before migration.</li> <li>Enhanced the legacy Claim Service Record (CSR) architecture to allow a read-only mode for safe access to pre-conversion data without training.</li> </ul>"},{"location":"resume/#system-analyst","title":"System Analyst","text":""},{"location":"resume/#2000-2010","title":"2000 - 2010","text":""},{"location":"resume/#team-lead-for-claims-service-record-csr","title":"Team Lead for Claims Service Record (CSR)","text":"<ul> <li>Led and owned maintenance of critical CSR applications across transaction management, architecture, security, data movement, organizational data, and automation tools.</li> <li>Streamlined system audits, security reviews, and performance troubleshooting as the single point of contact.</li> <li>Championed architecture, design, and code reviews to ensure high quality and consistency across the CSR application suite.</li> <li>Provided quality control and oversight as a State Farm contact and team lead for external associates in a support structure with only 10 State Farm internal employees for legacy CSR.</li> </ul>"},{"location":"resume/#claim-service-record-csr-project-highlights","title":"Claim Service Record (CSR) Project Highlights","text":"<ul> <li>Core team member on the HP CSR port project to re-platform to HPUX. </li> <li>Worked on HP server clustering effort to reduce server footprint by 40%.</li> <li>Contributed to the expansion of catastrophe services capacity by building a second centralized cluster.</li> <li>Involved in all projects from 2004 until 2012 as a design team member. Design team members functioned as architects and consultants for requirements, design, and code reviews.</li> </ul>"},{"location":"resume/#innovative-solutions","title":"Innovative Solutions","text":"<ul> <li>Developed and implemented a JDBC-based distributed query tool, enabling ad-hoc reports on 300 nightly-backup databases with no production impact while allowing reports for business to run during the day.</li> <li>Enhanced distributed command-line utilities for parallel execution and variable substitution across 300+ servers by server type, boosting operational efficiency.</li> </ul>"},{"location":"resume/#education","title":"Education","text":""},{"location":"resume/#the-american-college-bryn-mawr-pennsylvania","title":"The American College, Bryn Mawr, Pennsylvania","text":""},{"location":"resume/#2010","title":"2010","text":"<ul> <li>Chartered Financial Consultant (ChFC)</li> </ul>"},{"location":"resume/#indiana-state-university-terre-haute-indiana","title":"Indiana State University, Terre Haute, Indiana","text":""},{"location":"resume/#2000","title":"2000","text":"<ul> <li>B.S. Management Information Systems, minor Computer Science</li> <li>Graduated Magna Cum Laude</li> </ul>"},{"location":"blog/2023/09/09/ai-indemnification/","title":"AI Indemnification","text":"<p>Microsoft has agreed to indemnify users of its Copilot service from copyright claims with its new \"Copilot Copyright Commitment.\" This development may ease decision-makers' minds on potential copyright claims stemming from using the service.</p> <p>In their announcement, Microsoft notes this is an extension of existing indemnification for potential patent claims while using Microsoft products.</p> <p>Sometimes you get what you pay for, whether that is support or indemnification from various claims.</p> <p>Learn more from the link in the comments section.</p> <p>Microsoft announces new Copilot Copyright Commitment for customers</p>"},{"location":"blog/2025/03/01/agi/","title":"AGI","text":"<p>When someone claims we\u2019re on the brink of AGI...</p> <p>\"Camelot! Camelot!\"</p> <p>... The reality?</p> <p>\"It\u2019s only a model.\"</p> <p>AI tools are powerful, sure. But true intelligence? That\u2019s still a fantasy.</p> <p></p>"},{"location":"blog/2023/07/16/ai-principles/","title":"AI Principles","text":"<p>The ACM Technology Policy Council has published principles for generative AI. While good governance may not be as exciting as delivering new features, it is crucial for practitioners to act responsibly and thoughtfully in their work.</p> <p>The most essential message in the document is listed last:</p> <p>\"Accountability and responsibility: Public and private bodies should be held accountable for decisions made by algorithms they use, even if it is not feasible to explain in detail how those algorithms produced their results.\"</p> <p>Think of it as owning a self-driving car: you may not understand every line of code, but you\u2019re still responsible for where it takes you.</p> <p>For the full document, browse through the following link: ACM Generative AI Principles</p>"},{"location":"blog/2023/07/31/ai-prompt-injection-attacks/","title":"AI Prompt Injection Attacks","text":"<p>Bruce Schneier has hit the nail on the head in his recent post on AI prompt injection attacks. Schneier feels it\u2019s not possible to fully secure large language models (LLMs) against this kind of attack. Essentially, you can use AI to generate injection prompts, but read the article to learn more.</p> <p>Automatically Finding Prompt Injection Attacks</p>"},{"location":"blog/2024/02/18/analog-computing/","title":"Analog Computing","text":""},{"location":"blog/2024/02/18/analog-computing/#old-time-rock-roll","title":"Old-Time Rock &amp; Roll","text":"<p>Bob Seger released the song \"Old Time Rock &amp; Roll\" on his 1978 album Stranger in Town. Over the last year, several articles have highlighted a resurgence in analog computing \u2014 a mode of computing that fell out of favor around the same time.</p> <p>From Wikipedia:</p> <p>\"An analog computer or analogue computer is a type of computer that uses the continuous variation aspect of physical phenomena such as electrical, mechanical, or hydraulic quantities (analog signals) to model the problem being solved.\"</p> <p>Analog computing is gaining renewed interest due to its energy efficiency. It\u2019s like vinyl records making a comeback \u2014 sometimes the old ways have a charm and practicality that modern methods can\u2019t match. Companies like Mythic are even developing Analog Matrix Processors (AMPs) for AI applications!</p> <p>Learn more about these developments from the Association for Computing Machinery journal Communications of the ACM: cacm.acm.org</p>"},{"location":"blog/2025/06/14/balancing-urgency-with-curiosity/","title":"Balancing Urgency with Curiosity","text":"<p>Not all urgency is useful. Effective leaders pause, stay curious, and prioritize doing the right thing ... even if it takes longer.</p> <p>How smart leaders balance urgency with curiosity</p>"},{"location":"blog/2025/04/22/bridging-the-knowledge-chasm/","title":"Bridging the Knowledge Chasm","text":"<p>For many seasoned professionals in tech, \u201cWe\u2019re not gonna take it\u201d isn\u2019t just a rock anthem \u2014 it\u2019s the quiet refrain of those who\u2019ve carried the weight of critical systems, late-night incidents, and relentless change.</p> <p>Everyone retires eventually. But the knowledge chasm beyond the retirement cliff for those who\u2019ve built, run, and sustained mission-critical systems is a growing crisis for many technology teams and the business functions they enable.</p> <p>These professionals aren\u2019t just skilled workers; they\u2019re stewards with profound system knowledge, historical context, and the undocumented \u201cwhy\u201d behind how things actually work.</p> <p>When they leave, they often take decades of insight with them. What remains is usually a brief, semi-structured handoff, if that.</p> <p>We need a better bridge across the knowledge chasm.</p> <p>Flexible employment options \u2014 ike part-time roles, short-term consulting, and targeted mentorship \u2014 can help build that bridge without overloading people approaching retirement.</p> <p>Remote work should absolutely be on the table. Many would gladly contribute if they could do it from closer to family or even while finally traveling.</p> <p>Pair that flexibility with a clear, personalized knowledge transfer plan. Define goals and structure the engagement to make the most of every hour shared between mentor and mentee.</p> <p>Most mentors want to share their experience. In conversations with colleagues near retirement, I hear it again and agaiw \u2014 there\u2019s real pride in passing on what they know.</p> <p>But mentors require the time, space, and structure to do it right. And just as importantly, they require mentees \u2014 people committed, assigned, and accountable to learn and carry that knowledge forward.</p> <p>Flexible transition arrangements aren\u2019t just kind \u2014 they\u2019re strategic. They won\u2019t be needed for every retirement, but they\u2019re an option to provide continuity, reduce risk, and avoid the cost of overlapping two full-time roles.</p> <p>Offered early, they encourage open conversations about retirement timing \u2014 giving teams the time to prepare rather than scramble.</p> <p>Let\u2019s listen and respond with thoughtful, flexible solutions that honor experience and protect essential systems and institutional knowledge.</p>"},{"location":"blog/2025/03/23/circuit-breaker-pattern/","title":"Circuit Breaker Pattern","text":"<p>Imagine forgetting your electric tea kettle was on. No big deal ... it shuts itself off when it senses danger, like potentially burning your house down. This is the circuit breaker pattern in action.</p> <p>When a service starts failing, you don't keep hammering it. Instead, you back off, stop calling it for a while, and let it recover.</p> <p>Simple, right? Well, as with most things in engineering, it depends.</p> <ul> <li> <p>Should my kettle retry a few times before giving up? Nope. In this case, retrying is dangerous. But for a network request? Maybe! That\u2019s max retries with exponential backoff giving a service time to recover without making things worse.</p> </li> <li> <p>What if my kettle completely fails? Do I just give up on tea? Of course not. I grab a pot and boil water on the stove, or I make coffee. That\u2019s a fallback strategy like calling a secondary service, serving cached data, or skipping a step if it\u2019s not critical.</p> </li> </ul> <p>So, should your system trip a circuit breaker and stop retrying? It depends. On the failure mode, the risk, and the alternatives. Context matters.</p> <p>My tea kettle gets it. Maybe our systems should too?</p>"},{"location":"blog/2024/02/18/colossus/","title":"Colossus","text":""},{"location":"blog/2024/02/18/colossus/#the-first-digital-computer","title":"The First Digital Computer","text":"<p>The Colossus, created 80 years ago for code-breaking during World War II, holds the distinction of being the first digital computer.</p> <p>One fascinating aspect of its history is a memo describing the Colossus as one of two competing options. While the memo acknowledged it as a \"much more ambitious scheme,\" the recommendation was to pursue both options.</p> <p>This \"both-and\" thinking preserved optionality, which ultimately led to the Colossus playing a pivotal role in convincing Hitler that the Allies would invade from Pas De Calais rather than Normandy.</p> <p>Learn more in the following GCHQ article: GCHQ celebrates 80 years of Colossus</p>"},{"location":"blog/2024/03/25/quoted-names/","title":"Quoted Names","text":"<p>Quoted database object names are like getting dysentery on the Oregon Trail </p> <p>... It\u2019s not gonna end well.</p>"},{"location":"blog/2024/02/18/dba-roles/","title":"DBA Roles","text":""},{"location":"blog/2024/02/18/dba-roles/#on-forgetting-things","title":"On Forgetting Things","text":"<p>Adrien Nayrat has an excellent article that delves into what many have forgotten about the DBA (Database Administrator) role.</p> <p>I believe Mr. Nayrat is speaking to a knowledge gap many teams have to continually jump over as they work on deploying or refactoring solutions.</p> <p>Mr. Nayrat\u2019s article is well worth your time:</p> <p>Postgres again elected DBMS of the Year in 2023, but I'm worried</p>"},{"location":"blog/2025/03/27/design-patterns/","title":"Design Patterns","text":"<p>Design patterns didn\u2019t start with software. Before the Gang of Four introduced them to software engineering, Christopher Alexander used patterns to explain how to make cities and buildings more livable.</p> <p>His 1977 book, A Pattern Language, revealed how timeless design principles create spaces where people thrive.</p> <p>The same thinking has made software systems more adaptable, maintainable, and human-friendly. Even if you don\u2019t know their names, you\u2019ve likely worked with these patterns.</p> <p>The first system I supported was a distributed COBOL application written about a decade before AWS existed. Technically a microlith, it needed to handle distributed transactions without two-phase commit. To do this it had a homegrown transaction manager, implementing what we now call the outbox pattern. It handled retries with a circuit breaker. Not only that, but it even offered functionality that resembled step functions.</p> <p>It just worked. And that\u2019s the point. Good systems, like good cities, are livable.</p> <p>The tools to make them that way already exist. Are you using them?</p> <p>Learn more about A Pattern Language</p>"},{"location":"blog/2025/06/16/the-evolution-of-fourth-generation-languages-4gls/","title":"The Evolution of Fourth Generation Languages (4GLs)","text":"<p>The ability to describe what you want and get it without knowing the implementation details started with Fourth Generation Languages (4GLs) like SQL. SQL emerged in the 1970s and was commercialized alongside other 4GLs in the 1980s.</p> <p>For example, SELECT * FROM users WHERE age &gt; 25 expresses what you want rather than requiring you to write loops and conditionals to iterate through records.</p> <p>Of course, 4GLs still required specific syntax and were limited to specific domains. GenAI, by contrast, allows natural language and spans a much broader set of domains.</p> <p>While GenAI represents a significant change, it is also a continuation of a long arc in computing towards democratization. This evolution excites me, as it promises broader participation in technology.</p> <p>For instance, individuals without deep technical backgrounds can now ask for code examples or request explanations without needing to know low-level details. GenAI\u2019s usefulness extends beyond coding, with summarization proving valuable in various contexts.</p> <p>Certainly, there will still be implementation details and mistakes to address. Buggy code, security vulnerabilities, and logic errors will persist. However, I hope this technology will make technology more approachable and empowering for many.</p> <p>AI-powered summaries can assist small businesses in automating routine tasks and enable humanities students to explore data confidently without requiring deep expertise in R or Python.</p> <p>While challenges remain, the potential for broader participation is a future worth anticipating.</p>"},{"location":"blog/2025/06/02/lessons-from-the-worlds-first-speeding-ticket/","title":"Lessons from the World's First Speeding Ticket","text":"<p>In 1896, Walter Arnold achieved an unusual first: the world's first speeding ticket. He was caught going 8 miles per hour (mph) in a 2 mph zone in Paddockwood, Kent, UK.</p> <p>This speed limit seems astonishingly low today. Why was it so slow?</p> <p>Early cars had poor brakes and unreliable steering. Traffic signals didn\u2019t exist, roads were chaotic, and were shared with startled horses and pedestrians. Driver training was nonexistent, and the infrastructure for faster travel simply wasn\u2019t there.</p> <p>The 2 mph limit reflected the world as it was before modern cars.</p> <p>What changed to allow faster, safer driving? It wasn\u2019t just about faster cars; it was about better systems:</p> <ul> <li>Engineering for cars and roads saw continuous improvement.</li> <li>Regulations evolved to manage new risks automobiles introduced.</li> <li>Driver training became mandatory.</li> <li>Controls were standardized (steering wheels, pedals, brakes), making every car more or less the same to operate.</li> </ul> <p>Driving became easier, safer, and faster because mechanical and civil engineering, law, and human-centered design advanced together.</p> <p>This example illustrates how innovation isn\u2019t just about a single invention. Instead, changes are required throughout the entire system around the innovation.</p> <p>Walter Arnold\u2019s ticket also offers another lesson. He wasn\u2019t just a driver; he was also a car dealer. Historians suggest the ensuing publicity provided a significant boost to his business.</p> <p>Perhaps not such an innocent ticket after all!</p> <p>Sources: </p> <ul> <li>Guinness World Records: First person charged with a speeding offence</li> <li>Historic UK: Walter Arnold \u2013 World's First Speeding Ticket</li> <li>Snopes: Walter Arnold and the First Speeding Ticket</li> </ul>"},{"location":"blog/2025/03/15/fit-for-purpose/","title":"Fit For Purpose","text":"<p>While I start my day with coffee, I\u2019ve been sipping hot tea for the rest of it lately. (Exciting, right?) But there\u2019s a bigger point here.</p> <p>My gas stove is very slow at boiling water, so I looked into switching to an induction stove. It would be faster and more efficient, but my kitchen isn\u2019t wired for it. Plus, buying an entirely new stove would be expensive and, honestly, over-engineered for solving the problem of boiling water faster.</p> <p>Instead, I spent $15 on an electric kettle. It solved the problem quickly, without a hefty price tag.</p> <p>Why should you care?</p> <p>Think about a problem you\u2019re facing:</p> <ul> <li>Do you truly understand the real problem?</li> <li>Is there a simple solution?</li> </ul> <p>While this example may seem trivial, it\u2019s a reminder that knowing exactly what problem you\u2019re solving is key to finding solutions that are fit for purpose. Moreover, the simplest and most direct solution is often the most cost-effective one.</p>"},{"location":"blog/2024/12/22/foreign-keys/","title":"Foreign Keys","text":"<p>I learned something new today about Postgres!</p> <p>Laurenz Albe\u2019s blog post on how you can break foreign keys in Postgres includes the tidbit that BEFORE triggers that return NULL can stop enforcement of foreign keys.</p> <p>While I knew system triggers enforced referential integrity in Postgres, I had not considered the consequences of BEFORE triggers with that feature.</p> <p>Learn more from Laurenz Albe here:</p> <p>Broken Foreign Keys in Postgres</p>"},{"location":"blog/2023/07/02/generated-pipelines/","title":"Generated Pipelines","text":"<p>I want to talk a little about code generation.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#avoid-drudgery","title":"Avoid Drudgery","text":"<p>One example of code generation is this blog. It is hosted on Github Pages and originally used a tool called Jekyll to render simple Markdown documents as static webpages.</p> <p>Jekyll removes the need to write a large amount of boilerplate HTML and allows you to focus on writing.</p> <p>This blog, though, is about generating Gitlab CI child pipelines.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#companion-project","title":"Companion Project","text":"<p>There is a companion pipeline-welder project that has all the source code for this blog. While it is a toy project, it is a working example for a Python + Jinja way to generate pipelines. We will focus on generating Gitlab CI pipelines.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#template-engines","title":"Template Engines","text":"<p>All programming languages have code generation capabilities, but many languages also have tools that can render templates. These tools can evaluate variables, functions, conditionals, loops, etc.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#jinja-engine","title":"Jinja Engine","text":"<p>Jinja is a Python template engine that has these capabilities and more. The full Jinja template language is quite extensive, but we will only make use of conditional includes and variables.</p> <p>Hopefully, by limiting ourselves, it will make the concepts easier to follow.</p> <p>Specifically, our Jinja template will:</p> <ul> <li>Generate a pipeline with build, test, and deploy stages</li> <li>The deploy stage will be optional and included only if the deploy job is requested</li> <li>The test stage will also have an optional lint job</li> </ul> <p>You might note the <code>rules</code> command and variables in Gitlab CI would let you do the same things without using a template library.</p> <p>While this is a contrived example to present the concept of generating and triggering a pipeline, there are cases where being able to do this can make your code simpler.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#jinja-template","title":"Jinja Template","text":"<p>What does a Jinja template look like?</p> <p>We will use the following template:</p> <pre><code>{% raw %}\n{# Sample pipeline in jinja format -- this is a comment #}\nstages:\n  - build\n  - test\n{% if deploy %}\n  - deploy\n{% endif %}\n\nbuild-job:\n  stage: build\n  script:\n    - echo \"{{ build_command }}\"\n\nunit-test-job:\n  stage: test\n  script:\n    - echo \"{{ test_command }}\"\n\n{% if lint %}\nlint-test-job:\n  stage: test\n  script:\n    - echo \"Linting ...\"\n{% endif %}\n\n{% if deploy %}\ndeploy-job:\n  stage: deploy\n  environment: {{ environment_name }}\n  script:\n    - echo \"Deploying application...\"\n{% endif %}\n{% endraw %}\n</code></pre>"},{"location":"blog/2023/07/02/generated-pipelines/#default-behavior","title":"Default Behavior","text":"<p>In general, Jinja will simply output the inline text.  Jinja also has a tagging syntax using <code>{% raw %}{ }{% endraw %}</code> braces.  </p> <p>Tags are how we will tell Jinja what actions to take to modify output while rendering our template.</p> <p>Jinja evaluates the text between the braces to allow comments, conditional execution, variable replacement, loops, etc.  We will only look at the first three.  </p> <p>For a full listing of features, see the Jinja website!</p> <p>This section shows a comment:</p> <pre><code>{% raw %}\n{# Sample pipeline in jinja format -- this is a comment #}\n{% endraw %}\n</code></pre> <p>This is an example of an if block that will include the nested line if the expression is true.</p> <pre><code>{% raw %}\n{% if deploy %}\n  - deploy\n{% endif %}\n{% endraw %}\n</code></pre> <p>Finally, this is an example of variable replacement.  The space between the braces will be replaced with the value of <code>build_command</code>.  Note, the braces are also replaced as they are just tags for Jinja to find, evaluate, and replace.</p> <pre><code>{% raw %}\n    - echo \"{{ build_command }}\"\n{% endraw %}\n</code></pre>"},{"location":"blog/2023/07/02/generated-pipelines/#project-structure","title":"Project Structure","text":"<p>Our companion pipeline-welder project has the following directory structure:</p> <pre><code>.\n\u251c\u2500\u2500 templates\n\u2502   \u2514\u2500\u2500 jinja_sample.txt\n\u251c\u2500\u2500 welder\n\u2502   \u2514\u2500\u2500 tests\n\u2502   |   \u2514\u2500\u2500 welder_test.py\n\u2502   \u251c\u2500\u2500 LICENSE.txt\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __main__.py\n\u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 welder.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .gitlab-ci.yml\n\u251c\u2500\u2500 LICENSE.txt\n\u251c\u2500\u2500 README.md\n</code></pre> <p>You are welcome to use the code as you see fit.  It is licensed under the MIT license.  Just keep in mind this is a toy project and would need more work for a production setting.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#project-flowchart","title":"Project Flowchart","text":"<p>As a part of running the <code>.gitlab-ci.yml</code> pipeline, Gitlab will start two jobs <code>plan</code> and <code>apply</code>.</p> <p>First, the plan job will run the welder.py program, which will receive command line inputs.  We will request the optional deploy job be included.  However, we will not request linting; so this optional job will not run.  Additionally, we will provide commands to run during the build and test jobs of the generated pipeline as well as the environment name for the deploy job.</p> <p>The Jinja Engine will combine the template with the inputs when we call the <code>render()</code> function.  After rendering, we will write the generated pipeline output to a file.</p> <p>Second, the apply job will use the trigger command to submit the generated pipeline to run independently as a child pipeline.</p> <p></p>"},{"location":"blog/2023/07/02/generated-pipelines/#gitlab-screens","title":"Gitlab Screens","text":"<p>This is what you see within Gitlab for the project's <code>.gitlab-ci.yml</code> when it runs.</p> <p></p> <p>Only the plan and apply jobs are part of the parent pipeline.  The apply job itself creates the Downstream pipeline by issuing a <code>trigger</code> command.  You can click through the Downstream pipeline to see the generated, child pipeline.</p> <p></p>"},{"location":"blog/2023/07/02/generated-pipelines/#parent-pipeline","title":"Parent Pipeline","text":"<p>What does the parent <code>.gitlab-ci.yml</code> code look like?</p> <pre><code>stages:\n    - plan\n    - apply\n\nplan:\n  stage: plan\n  image: python:latest\n  script: \n    - python -m venv venv\n    - . venv/bin/activate\n    - pip install -r welder/requirements.txt\n    - python welder/welder.py \n      -t templates/jinja_sample.txt \n      -o generated_job.yml \n      -k build_command=\"Can we build it?\" \n      test_command=\"Yes we can!\" \n      environment_name=unit \n      lint=False \n      deploy=True  \n      -v\n    - pwd;ls -l\n\n  artifacts: \n    paths: \n      - ./generated_job.yml\n\napply:\n  stage: apply\n  trigger:\n    include:\n      - artifact: generated_job.yml\n        job: plan\n    strategy: depend\n</code></pre> <p>The use of <code>image: python:latest</code> on gitlab.com is ok for a toy project.  It would be better for a production environment to build your own image where you can control the versions of software in the image.</p> <p>First, the plan job sets up a python virtual environment and installs dependencies.  Next, the <code>welder.py</code> program runs and receives several arguments:</p> <ul> <li><code>-t</code> <ul> <li>This is the Jinja template file</li> </ul> </li> <li><code>-o</code> <ul> <li>This is the output file </li> </ul> </li> <li><code>-k</code> <ul> <li>This is a list of key=value options<ul> <li>The following inputs are passed as key=value pairs for replacement:</li> <li>build_command=\"Can we build it?\"</li> <li>test_command=\"Yes we can!\"</li> <li>environment_name=unit</li> <li>The following inputs are passed as key=value pairs for controlling which jobs to include:</li> <li>lint=False </li> <li>deploy=True  </li> </ul> </li> </ul> </li> <li><code>-v</code> <ul> <li>This is a verbose option</li> </ul> </li> </ul> <p>After the python program exits, the <code>artifacts</code> command is used to save the output from the python script as an artifact named <code>./generated_job.yml</code>.</p> <p>Finally, the apply job uses the <code>trigger</code> command to submit a child pipeline with the artifact named <code>./generated_job.yml</code>.  It uses a <code>strategy: depend</code> telling the parent pipeline to wait for the child pipeline to complete.</p> <p>So, while the child pipeline runs independently of the parent, the parent will wait for completion of the child before exiting.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#plan-job","title":"Plan Job","text":"<p>If we peek inside the plan job in the parent pipeline, we can see what the job run looked like, including the <code>generated_pipeline.yml</code> artifact being saved.</p> <p></p> <p></p>"},{"location":"blog/2023/07/02/generated-pipelines/#artifacts","title":"Artifacts","text":"<p>Artifacts are outputs from jobs that Gitlab will save and optionally pass to other commands.</p> <p>To view the <code>generated_job.yml</code> artifact, on the right-hand side of the job window, we can choose an option under Job Artifacts: \"Download\" to download a zip file of all artifacts or \"Browse\" to see a list of artifacts.  We will choose \"Browse\".</p> <p></p> <p>From the \"Browse\" screen, we can click on the artifact and choose to download the <code>generated_job.yml</code> file to our workstation.</p> <p></p> <p>Below are the contents of the downloaded <code>generated_job.yml</code> file.  We can see that variable replacement and conditional includes have been evaluated based on the inputs for <code>welder.py</code> above.</p> <pre><code>stages:\n  - build\n  - test\n  - deploy\n\nbuild-job:\n  stage: build\n  script:\n    - echo \"Can we build it?\"\n\nunit-test-job:\n  stage: test\n  script:\n    - echo \"Yes we can!\"\n\n\ndeploy-job:\n  stage: deploy\n  environment: unit\n  script:\n    - echo \"Deploying application...\"\n</code></pre>"},{"location":"blog/2023/07/02/generated-pipelines/#python-welderpy","title":"Python welder.py","text":"<p>The <code>welder.py</code> program is what is calling Jinja. </p> <ul> <li>First, <code>main()</code> is called to handle command line arguments  </li> <li>Second, <code>gitlab_jinja()</code> is called to set up Jinja and call <code>render()</code> </li> <li>Finally, output is written to a stdout or a file </li> </ul> <p>Here is the code in its entirety.  If you prefer, you can view the code in the gitlab project here.</p> <pre><code>import argparse\nimport errno\nfrom jinja2 import Environment, FileSystemLoader\nfrom os.path import basename, dirname, exists, isfile, realpath\nimport os\nimport sys\n\n\ndef gitlab_jinja(template_file=None, output_file=None, verbose=None, **welder_kv):\n    \"\"\"gitlab jinja planner for welder pipeline assembler\n\n    Keyword arguments:\n    template_file -- the path to the template file to evaluate\n    output_file -- the generated gitlab pipeline\n    verbose -- verbose switch\n    welder_kv -- key-value for evaluation/replacement\n    \"\"\"\n    if not exists(template_file):\n        print(f\"ERROR: template_file '{template_file}' does not exist!\")\n        sys.exit(1)\n    elif not isfile(template_file):\n        print(f\"ERROR: template_file '{template_file}' is a directory!\")\n        sys.exit(1)\n\n    template_file_dir = dirname(realpath(template_file))\n    template_file_basename = basename(template_file)\n\n    environment = Environment(\n        loader=FileSystemLoader(template_file_dir),\n        trim_blocks=True,\n        lstrip_blocks=True,\n    )\n    template_file = environment.get_template(template_file_basename)\n\n    content = template_file.render(**welder_kv)\n\n    if output_file == \"stdout\":\n        sys.stdout.write(content)\n    else:\n        with open(output_file, mode=\"w\", encoding=\"utf-8\") as pipeline:\n            pipeline.write(content)\n\n    if verbose == True:\n        print(f\"INFO: Created {output_file}\")\n\n\ndef main():\n    \"\"\"CLI for welder dynamic gitlab pipelines\"\"\"\n    parser = argparse.ArgumentParser(\n        prog=\"welder\", description=\"assemble dynamic gitlab pipelines\"\n    )\n\n    parser.add_argument(\"-o\", \"--output_file\", action=\"store\", default=\"stdout\")\n    parser.add_argument(\"-t\", \"--template-file\", action=\"store\", required=\"true\")\n    parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n    parser.add_argument(\n        \"-k\",\n        \"--keywords\",\n        nargs=\"+\",\n        action=\"append\",\n        help=\"keyword pairs with quotes for whitespace\",\n    )\n\n    args = parser.parse_args()\n\n    if not exists(args.template_file):\n        print(f\"ERROR: template '{args.template_file}' does not exist!\")\n        sys.exit(1)\n\n    if exists(args.output_file) and args.output_file != \"stdout\":\n        print(f\"WARNING: output_file file '{args.output_file}' will be overlaid!\")\n\n    welder_kv = {}\n    for key_pair_list in args.keywords:\n        for key_pair in key_pair_list:\n            key_list = key_pair.split(\"=\")\n            if len(key_list) &gt; 1:\n                if key_list[1].upper() == \"FALSE\":\n                    welder_kv.update({key_list[0]: False})\n                elif key_list[1].upper() == \"TRUE\":\n                    welder_kv.update({key_list[0]: True})\n                elif isinstance(key_list[1], int):\n                    welder_kv.update({key_list[0]: int(key_list[1])})\n                elif isinstance(key_list[1], float):\n                    welder_kv.update({key_list[0]: float(key_list[1])})\n                else:\n                    welder_kv.update({key_list[0]: key_list[1]})\n            else:\n                welder_kv.update({key_list[0]: True})\n\n    if args.verbose:\n        print(\n            f\"\"\"\n            Welder Input:\n            template-file={args.template_file}\n            output_file={args.output_file}\n            verbose={args.verbose}\n            keywords={args.keywords}\n            welder_kv={welder_kv}\n            \"\"\"\n        )\n\n    gitlab_jinja(args.template_file, args.output_file, args.verbose, **welder_kv)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"blog/2023/07/02/generated-pipelines/#lines-of-code","title":"Lines of Code","text":"<p>As you can see from scanning the code above, most of the code in this  program is for handling command line options in <code>main()</code>.</p> <p>The second-largest amount of code is for file I/O in <code>gitlab_jinja()</code> where we validate the template exists, format the path for the template directory, and later write the rendered template to stdout or a file.</p> <p>I also noted while writing the blog that I have duplicate file existence checks in <code>main()</code> and <code>gitlab_jinja()</code>.  While they do need to be checked in both places in case the function is called outside the main(), the checks could be refactored into a function that raises an exception that could be used in both places.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#crazy-eights","title":"Crazy Eights","text":"<p>Only the following 8 lines of code in <code>gitlab_jinja()</code> are directly related to rendering templates.</p> <pre><code>    environment = Environment(\n        loader=FileSystemLoader(template_file_dir),\n        trim_blocks=True,\n        lstrip_blocks=True,\n    )\n    template_file = environment.get_template(template_file_basename)\n\n    content = template_file.render(**welder_kv)\n</code></pre> <ul> <li>First, the <code>Environment()</code> function loads the template directory as a source and tells Jinja what we want to do with whitespace</li> <li>Second, the <code>get_template()</code> function does exactly what it sounds like it does</li> <li>Finally, the <code>render()</code> function receives a key value dictionary with inputs from the command line and transforms the template into content</li> </ul> <p>So, the code required to render a template is minimal.</p>"},{"location":"blog/2023/07/02/generated-pipelines/#future-research","title":"Future Research","text":"<p>I hope I have made you more interested in Jinja and generated child pipelines in Gitlab.  When used appropriately, both can simplify workflows.</p> <p>We barely touched on Jinja's extensive feature set, and you also get all the other things Python can do when you're using Jinja.</p> <p>Happy coding!</p>"},{"location":"blog/2025/04/04/statistics-and-guinness/","title":"Statistics and Guinness","text":"<p>Tech is Cool, But Results Rule</p> <p>Guinness doesn\u2019t just brew beer! One of their statisticians, William Sealy Gosset (aka \u201cStudent\u201d), developed the t-test to improve quality control while keeping sample sizes small.</p> <p>A perfect model, elegant design, or aesthetic bit of code means nothing if it doesn\u2019t solve a problem.</p> <p>Impact is what matters. Cheers to making a difference!</p> <p>Find out more with the story below:</p> <p>How the Guinness Brewery Invented the Most Important Statistical Method</p>"},{"location":"blog/2023/09/09/human-in-the-loop/","title":"Human In The Loop","text":"<p>What if an AI monitored whether you followed all rules and laws that applied to you?</p> <p>This chilling and dystopian thought is the subject of a guest post by Jon Penney on Bruce Schneier\u2019s blog (link at bottom).</p>"},{"location":"blog/2023/09/09/human-in-the-loop/#key-points","title":"Key Points","text":"<ol> <li>The article is a good reminder of why there should be a \u201chuman in the loop\u201d for higher-risk activities. This is not a new concept. Consider nuclear weapon launch procedures. Multiple people must carry out any order to launch because there are serious consequences.</li> </ol> <p>Accusing someone of an illegal activity could carry serious consequences and should require a human in the loop.</p> <ol> <li>Penney covers the legal risks to the targets of AI-based decisions, but one would hope legal risks will also exist for the owners of misbehaving AI systems.</li> </ol> <p>While businesses need to carefully consider AI efforts on their technical and business merits, they also need to consider whether outcomes are fair and defensible in court.</p>"},{"location":"blog/2023/09/09/human-in-the-loop/#example","title":"Example","text":"<p>Take, for example, an AI system that identifies potential fraud. Is the AI that made the recommendation accurate and free of error and bias? Can the recommendations the AI makes be explained? Was an actual decision made by the AI, or was it only a recommendation reviewed against other information?</p> <p>Accusing someone of fraud is serious and could lead to a lawsuit or choosing not to fulfill an otherwise required action under a contract.</p> <p>I am not a lawyer, but to me, a human in the loop making decisions based on a recommendation seems more defensible. The human can review the recommendation and hopefully catch errors.</p> <p>What do you think? What systems need a human in the loop?</p> <p>AI and Micro-directives</p>"},{"location":"blog/2023/09/09/human-in-the-loop/#acm-generative-ai-principles","title":"ACM Generative AI Principles","text":"<p>The ACM Technology Policy Council agrees that responsibility is important. Human-In-The-Loop is just one type of responsibility. The ACM has published principles for generative AI. While good governance is less exciting than delivering new features, it is important for practitioners to be responsible and thoughtful in their work.</p> <p>The most essential message in the document is listed last:</p> <p>\"Accountability and responsibility: Public and private bodies should be held accountable for decisions made by algorithms they use, even if it is not feasible to explain in detail how those algorithms produced their results.\"</p> <p>For the full document, browse through the following link: ACM Generative AI Principles</p>"},{"location":"blog/2025/03/19/junior-dbas/","title":"Junior DBAs","text":"<p>The strongest junior DBAs for operational systems come from software development backgrounds with a genuine passion for data. </p> <p>They\u2019ve built apps, optimized queries, and experienced firsthand the pain of poor database decisions before transitioning into a DBA role.</p> <p>Why does this matter? A broader background means these DBAs are better at:</p> <ul> <li>Spotting bad design decisions</li> <li>Diagnosing root causes of performance issues</li> <li>Giving realistic application tuning advice</li> <li>Weighing tradeoffs in database designs</li> <li>Automating processes with solid coding skills</li> </ul> <p>If you\u2019re hiring a junior DBA, you\u2019re likely considering candidates from various IT roles. While SysAdmins and Data Analysts can thrive as DBAs, for operational systems, software developers often bring an extra edge.</p>"},{"location":"blog/2025/05/03/legacy-teams-dont-need-cake--they-need-a-seat-at-the-table/","title":"Legacy Teams Don\u2019t Need Cake \u2014 They Need a Seat at the Table","text":"<p>With modernization efforts, new systems often get the spotlight and celebrating those wins is important.</p> <p>But beware the \u201ccake in the break area\u201d trap.</p> <p>If your legacy support team\u2019s main interaction with your modernization effort is a leftover piece of cake, you\u2019ve fallen into that trap.</p> <p>Legacy engineers keep the present running while the future is still being built. They bring deep institutional knowledge, production support experience, and insight into why systems were built the way they were \u2014 knowledge earned through past rewrites, migrations, and upgrades.</p> <p>Many are mid-career, not just folks nearing retirement. They\u2019ve got years of impact ahead and can play a vital role in both today\u2019s operations and tomorrow\u2019s transformation.</p> <p>And yet, there\u2019s a persistent myth: that working on legacy systems means someone is stuck in the past.</p> <p>The reality? They\u2019re often the ones holding everything together.</p> <p>This misunderstanding leads to legacy teams being sidelined from modernization work. Not out of malice \u2014 but because people are busy, assumptions are made, and there\u2019s a (wrong) belief that they\u2019re not interested in what\u2019s next.</p> <p>That\u2019s a costly mistake, especially during a period of change.</p>"},{"location":"blog/2025/05/03/legacy-teams-dont-need-cake--they-need-a-seat-at-the-table/#if-youre-leading-modernization","title":"If You\u2019re Leading Modernization","text":"<p>Make deliberate choices to include legacy teams:</p> <ul> <li>Bring them in early. Their system knowledge can reduce risk. Ask them to find gaps.</li> <li>Explain why they matter. It\u2019s not just respectful, it\u2019s critical to success.</li> <li>Check in regularly. Interest in contributing can evolve. Use one-on-ones to uncover it.</li> <li>Make space for contributions. Consider how assignments or priorities can be adjusted to free people up.</li> <li>Avoid subtle slights. Talk about improvements, but don\u2019t treat \u201clegacy\u201d like a dirty word.</li> <li>Recognize the team keeping the lights on. Their wins matter, too.</li> </ul>"},{"location":"blog/2025/05/03/legacy-teams-dont-need-cake--they-need-a-seat-at-the-table/#if-you-support-legacy-systems","title":"If You Support Legacy Systems","text":"<p>And you want to help shape what\u2019s next:</p> <ul> <li>Track your workload. Be clear about your bandwidth. It helps your manager shift priorities as needed.</li> <li>Share your goals. Show how your experience supports modernization success.</li> <li>Collaborate openly. Offer insights and advocate for your inclusion.</li> <li>Upskill with purpose. Learn the tools and approaches shaping the future.</li> <li>Lead through change. If you\u2019ve seen transformation before, that\u2019s your superpower.</li> <li>Speak constructively. Frame concerns with context. Help others understand the \u201cwhy.\u201d</li> </ul> <p>Already stretched too thin to get involved? That\u2019s a real barrier, but it\u2019s also one leaders may be able to address. Have that conversation.</p> <p>However you choose to engage, your experience matters.</p> <p>The future is stronger with you in it.</p>"},{"location":"blog/2025/03/25/microservices--shared-data/","title":"Microservices: The Data Ownership Trap","text":"<p>Microservices promise agility and scalability, but there's a hidden pitfall that can derail your chosen architecture: shared data.</p>"},{"location":"blog/2025/03/25/microservices--shared-data/#the-silent-killer-shared-tables","title":"The Silent Killer: Shared Tables","text":"<p>Splitting your application into services isn't enough. If your services are still reaching into shared database tables, you're not building a true microservices architecture.</p>"},{"location":"blog/2025/03/25/microservices--shared-data/#red-flags-to-watch-out-for","title":"Red Flags to Watch Out For:","text":"<ul> <li>Direct table access across services</li> <li>Pseudo-independent copies of tables within the same operational system</li> <li>Triggers creating bottlenecks on shared tables</li> <li>Legacy stored procedures spanning multiple services</li> <li>The dangerous \"we'll fix it later\" mentality</li> </ul>"},{"location":"blog/2025/03/25/microservices--shared-data/#the-fundamental-principle-data-sovereignty","title":"The Fundamental Principle: Data Sovereignty","text":"<p>While there are exceptions, sharing tables across two or more microservices will create a tangled web of dependencies.</p>"},{"location":"blog/2025/03/25/microservices--shared-data/#the-path-to-true-microservices","title":"The Path to True Microservices:","text":"<ul> <li>Strict data ownership: Each service manages its own data.</li> <li>API-first communication: Services interact via APIs, not direct database access.</li> <li>Team alignment: Organize teams to match architectural boundaries (hello, Conway's Law!).</li> </ul>"},{"location":"blog/2025/03/25/microservices--shared-data/#the-bottom-line","title":"The Bottom Line","text":"<p>Good data design always included clear boundaries around data access; this was equally true with monoliths. </p> <p>Things you might have gotten away with before, though \u2026 like direct data access to update a shared table across two services inside a monolith \u2026 will not work with microservices.</p> <p>Microservices are complex. Your data boundaries must be as clear and intentional as your service boundaries.</p>"},{"location":"blog/2025/04/29/monty-python-and-the-holy-grail-turns-50/","title":"Monty Python and the Holy Grail Turns 50","text":"<p>Wow, 50 years of Monty Python and the Holy Grail!</p> <p>A perfect reminder that even the most absurd quests can teach us to laugh, persevere, and not take life too seriously.</p> <p>Just remember ... beware the killer rabbit.</p> <p>Read more at Ars Technica</p>"},{"location":"blog/2025/04/13/neglected-systems/","title":"Neglected Systems","text":"<p>It\u2019s tempting to view Mainframe, distributed, and cloud systems as a linear progression from old to new. But in reality, systems are bundles of trade-offs. Every platform, no matter how popular, comes with both strengths and weaknesses.</p> <p>You can choose one platform or follow a hybrid strategy, but the one thing you can\u2019t do is stop investing in the platforms you rely on.</p> <p>If you underinvest (or worse disinvest) in the platforms you depend on, don\u2019t be surprised when they become rigid, outdated, or harder to evolve. This applies equally to technology providers and any business with a meaningful tech footprint.</p> <p>Investment must include the people who run, maintain, and grow these systems. They\u2019re just as critical to your technology portfolio as the platforms themselves.</p> <p>You can\u2019t neglect talent development and then wonder why you can\u2019t hire. Without entry-level roles and mentorship, you\u2019re guaranteeing a talent gap in the future. There won\u2019t be mid-career experts a decade from now \u2026 at least not ones who aren\u2019t approaching retirement.</p> <p>The most dangerous systems aren\u2019t old or new. The most dangerous systems are neglected systems.</p> <p>It\u2019s time we stopped thinking about technology purely in terms of tech stacks and architecture. These aren\u2019t just platforms.</p> <p>They\u2019re ecosystems, and the people who live within them need care, investment, and opportunities to thrive.</p>"},{"location":"blog/2023/12/02/new-york-cybersecurity-regulation/","title":"New York Cybersecurity Regulation","text":"<p>If you do business within New York State, regulators have introduced changes to cybersecurity regulations for financial services companies.</p> <p>Both Bloomberg and The Wall Street Journal have summaries, but Bloomberg\u2019s article links to the revised regulations.</p> <p>Bloomberg</p> <p>Wall Street Journal</p>"},{"location":"blog/2023/08/06/personal-email-at-work/","title":"Stop It!","text":"<p>An interesting article giving yet another example of why you should not use a personal email for work correspondence!</p> <p>An Internet provider for the Mali domain has been receiving emails intended for military domains.  According to the article, emails sent from military systems block outgoing email to these domains, but obviously email sent from private providers would not do the same.</p> <p>While the information disclosed does not appear to include any classified information, the article indicates sensitive information was included in some emails.</p> <p>Do you use your work email for personal correspondence or vice versa?</p> <p>Please stop doing that.</p> <p>Typo sends millions of US military emails to Russian ally Mali</p>"},{"location":"blog/2025/05/21/is-it-the-system-or-the-platform/","title":"Is It the System or the Platform?","text":"<p>\u201cThe platform my system runs on is out-of-date.\u201d</p> <p>It\u2019s a common complaint \u2014 but is the real problem \u201cmy system\u201d or \u201cthe platform\u201d?</p> <p>Here\u2019s the hard truth: we often blame our tools when the real issue is how we\u2019re using them.</p> <p>As Pogo famously said, \u201cWe have met the enemy and he is us.\u201d</p> <p>Keep in mind that years of deferred technical debt might be your real issue. While it won\u2019t be 100 percent true, it is unlikely to be false.</p> <p>Technical debt shows up as Byzantine workarounds, limited options, overly complex designs, and a deep fear of change.</p> <p>Moving to a new platform might be necessary, but unless you understand what\u2019s actually wrong with your current system, you\u2019re likely to carry the same issues forward.</p>"},{"location":"blog/2024/02/07/postgres-weekly/","title":"Postgres Weekly","text":"<p>A common question from people moving from other databases to Postgres is how can I learn more?</p> <p>One of the great things about the Postgres community is the excellent documentation!</p> <p>While I don\u2019t have any affiliation, Postgres Weekly is a great way to learn something new and keep tabs on Postgres news.</p> <p>https://postgresweekly.com/</p>"},{"location":"blog/2025/06/05/preserving-creative-spirit/","title":"Preserving Creative Spirit","text":"<p>As children, we built forts out of pillows and cardboard boxes ... no approvals, no fear of failure, just imagination and play.</p> <p>Now, as adults, we optimize for uptime and reliability. Production must be stable.</p> <p>How can we preserve that creative spirit while ensuring safety?</p> <p>You can carve out space away from production:</p> <ul> <li>Sandbox environments</li> <li>Hack days</li> <li>Prototypes</li> </ul> <p>Or enable safe experiments within production:</p> <ul> <li>Pilot rollouts</li> <li>Feature flags</li> <li>A/B testing</li> </ul> <p>Ideally, you\u2019re doing both.</p> <p>How do you make space for safe exploration on your team? </p> <p>More importantly, how do you help people feel safe doing it?</p>"},{"location":"blog/2023/12/02/rational-expectations/","title":"Rational Expectations","text":"<p>\"If you give a man a fish, you feed him for a day. If you teach a man to fish, you feed him for a lifetime.\" ~ Anonymous </p> <p>While database engineers (and administrators) want to be helpful and answer questions, you should not expect your database engineer to:</p> <ol> <li>Make \u201cthe database\u201d work the way you think it works.</li> <li>Teach you how to use Google to answer what a particular SQL error means.</li> <li>Magically make database contention go away.</li> </ol>"},{"location":"blog/2023/12/02/rational-expectations/#prepare-investigate-report","title":"Prepare, Investigate, Report","text":"<p>It is important for software engineers to have some knowledge about \"the database\" to be able to do their work effectively and efficiently.</p> <p>Software Engineers should prepare for, investigate, and report issues correctly.</p> <p>\ud83d\udcd6 Prepare</p> <p>How do you prepare for an issue? You read a good book for your chosen database, subscribe to database blogs, or (heaven forbid!) look at a manual. If video or audio is your preferred media, find training videos or podcasts. Or experiment!</p> <p>Learning is how you prepare to handle issues.</p> <p>You need to understand what your database will do with your queries. Learn how transactions and locking work within your database. As a stretch goal, learn how to \"explain\" the queries you are using to find out the plan that will be followed when the database evaluates one.</p> <p>Moreover, learn how to view the table and index definitions for your database. This allows you to easily find current information about the database while programming or investigating errors.</p> <p>The same tool you are using to query your database can describe objects within the database using catalog tables. Catalog tables contain meta-data about the database including lists of tables, indexes, columns, data types, foreign keys, triggers, procedures, functions, etc.</p> <p>While you can query these catalog tables directly, tools like DBeaver can format this data into a tree view or DDL (data definition language).</p> <p>Of course, you should also know how to catch, log, and view SQL error messages.</p> <p>\ud83d\udd0e Investigate</p> <p>Follow the \u201c15-minute rule\u201d with errors and attempt to find the answer to your question independently for at least 15 minutes. Most can be answered using Google and are not \u201cdatabase issues\u201d.</p> <p>You should look at SQL errors to find an SQL code, state, error, or warning. You will find something you can search on Google. </p> <p>Digging into errors to understand them on your own will help you be a better developer. You will learn not only what is wrong with your code, but how to avoid the issue in the future!</p> <p>\ud83d\udcf0 Report</p> <p>If you need help, please be a good reporter and include: </p> <ul> <li>**W**ho (ID)</li> <li>**W**hat (full error message and SQL)</li> <li>**W**hen (date/time in UTC)</li> <li>**W**here (host and database name)</li> </ul> <p>If you include these in your report, you will find out the last of the \u201c5 **W**s\u201d \u2026 **W**hy?</p>"},{"location":"blog/2023/12/02/rational-expectations/#database-contention","title":"Database Contention","text":"<p>A particularly vexing question for database engineers is database contention. </p> <p>Database contention is vexing because it is normally NOT a database issue!</p> <p>The most likely cause of database contention is poor application design due to a lack of understanding how the database will run a particular workload.</p>"},{"location":"blog/2023/12/02/rational-expectations/#units-of-work","title":"Units of Work","text":"<p>The relevant concept is a \u201cunit of work\u201d within the database. A unit of work contains all the changes for a given database \u201ctransaction\u201d during the time span from the first change (insert, update, delete, merge) to the point where you (or a framework you use) commits those changes.</p> <p>Actually, the term transaction is probably used by itself more these days, but I wanted to mention both as you will see both terms in blogs and journals.</p> <p>Contention occurs when two different transactions are updating the same item(s) within the database at the same time.</p> <p>Let's assume you have two methods in two different web services. These APIs (Application Programming Interfaces) both update the same item in the database:</p> <ul> <li>API #1 issues an update \u2026 and does NOT commit</li> <li>API #1 calls API #2</li> <li>API #2 issues an update \u2026 a timeout occurs waiting for a lock</li> </ul> <p>Why did this happen?</p> <ul> <li>Both APIs connect to the database.</li> <li>Both APIs start a database transaction.</li> <li>Both APIs attempt to acquire locks held until commit.</li> </ul>"},{"location":"blog/2023/12/02/rational-expectations/#lock-contention","title":"Lock Contention","text":"<p>What does the locking look like in this scenario?</p> <ul> <li>API #1 will acquire a lock and does not commit.</li> <li>API #1 retains its lock and calls API #2.</li> <li>API #2 will attempt to acquire a lock on the same data and will wait on #1 to commit.</li> <li>API #2 will eventually hit a timeout (we hope).</li> </ul> <p>Situations like this are why poor application design is the first candidate for causing deadlocks or timeouts. There are many variations on this scenario, and most cannot be resolved by your database engineer.</p> <p>While your database engineer may be able to do something to reduce contention in some situations, good application design is the only way to avoid contention.</p> <p>The scenario above might seem contrived, but it is a common scenario even experienced software engineers stumble into when frameworks, database triggers, or database routines hide or obfuscate what is going on in the database.</p>"},{"location":"blog/2023/12/02/rational-expectations/#frameworks","title":"Frameworks","text":"<p>If you are using an Object Relational Mapping (ORM) or database persistence framework, you need to understand how it manages transactions within the database. </p> <ul> <li>When are transactions initiated and committed?  </li> <li>Does your framework ever create sub-transactions?</li> </ul> <p>With a sub-transaction, the parent transaction can acquire locks that a child sub-transaction may have to wait on to be released. The dilemma is that when a sub-transaction is initiated, the parent is suspended. So it cannot release locks until the child returns.</p> <p>One example of this is Spring JDBC when using REQUIRES_NEW, but the situation is not limited to Spring, or JDBC. While there can be cases where sub-transactions are appropriate, it does make reasoning about transaction boundaries more difficult. Think twice before using sub-transactions.</p>"},{"location":"blog/2023/12/02/rational-expectations/#database-triggers-and-routines","title":"Database Triggers and Routines","text":"<p>Another place with hidden functionality is within database triggers and routines. While called triggers and routines are normally part of the calling application's transaction, they represent \"hidden code\" that can modify the database. </p> <p>Triggers are database objects that \"fire\" on certain events. For example, you can create a trigger to insert into a history table when a table is modified. Those triggers might call database routines, or may have inline routines to make changes directly to the database. Triggers are normally defined by database administrators.</p> <p>Database routines are functions and stored procedures that can be called to make changes within the database. They represent code external to the main application that is installed within the database and callable by one or more applications. These routines are often written by a smaller subset of developers, or may be maintained by your database administrator.</p> <p>Because the individuals with knowledge of how triggers and routines work are frequently not the developers writing your business logic, their functionality is typically hidden from team members not familiar with them. This can lead to issues in application design. </p> <p>In our contrived locking example, it is possible that the developers were not modifying the same item in the database directly in their code. The change to the same database item could have been caused by a trigger updating a table common to both APIs.</p> <p>There are cases where triggers and routines are needed for performance, auditing, or data integrity.</p> <p>To tell if your code base makes use of them, you should be able to view them in the same tool you run SQL. As long as the routines are written in a flavor of SQL/PL you should be able to view them, but some databases allow external procedures written in C, COBOL, etc. In those cases, you would want to speak with your database administrator to find out where the code for those is located.</p> <p>Database triggers and routines bring us full circle. These are items that you may need your database administrator's help to understand.  However, you should know if they exist and what they do to make good design choices.</p>"},{"location":"blog/2023/12/02/rational-expectations/#closing-thoughts","title":"Closing Thoughts","text":"<p>It is not uncommon for database contention to lead to system outages. Work to avoid that in your code by seeking understanding.</p> <p>Try to start learning more by subscribing to a top database blog: Feedly Top Database Blogs</p>"},{"location":"blog/2023/08/06/reputation-risk/","title":"Reputation Risk","text":"<p>Wired recently published a piece on API vulnerabilities in the Points platform used by many hotels, airlines, and banks.  One of the researchers pointed out the vulnerabilities would have had \u201ca cascading effect to every company utilizing their loyalty backend\u201d.  </p> <p>\u201cIt takes a lifetime to build a good reputation, but you can lose it in a minute.\u201d ~ Will Rogers</p> <p>It is an interesting thought experiment to wonder whose reputation would have been damaged more if the vulnerability was exploited and customers' information or points were stolen.  While I believe the Points API provider would be called out, the damage to the reputation of the airlines, hotels, and banks would have been greater.</p> <p>Do you disagree that customers will blame the companies who they deal with directly when a third-party exposes their data?</p> <p>Hackers Could Have Scored Unlimited Airline Miles by Targeting One Platform</p>"},{"location":"blog/2025/04/18/research-spikes/","title":"Research Spikes","text":"<p>Just as scientists choose experiments to validate theories, strong technical leaders use targeted research to uncover what lies beneath the surface.</p> <p>To make your research spikes count, aim beyond the obvious.</p> <ul> <li>Explore Intersections: Look where infrastructure, user experience (UX), and data flows converge. Systemic issues often emerge at the edges.</li> <li>Validate Assumptions: Use early research to pressure-test architectural decisions that may no longer serve your evolving system.</li> <li>Trace Bottlenecks: Minor slowdowns can signal deeper structural flaws. Don\u2019t stop at the symptom \u2014 follow the signal to the source.</li> </ul> <p>Consider how this plays out in a real-world scenario like platform migration. While a spike on session limits might confirm server needs, it can also reveal scaling constraints, challenge outdated assumptions, and reshape service boundaries.</p> <p>That\u2019s systems thinking: understanding how local choices ripple through your entire architecture and using that insight to build smarter.</p> <p>Tactical spikes solve problems. Strategic spikes reveal broader issues.</p> <p>The best technical leaders don\u2019t just solve problems. They see the whole system and reshape it. That\u2019s how resilient systems are built.</p>"},{"location":"blog/2025/03/22/willing-to-fail/","title":"Willing to Fail","text":"<p>Being willing to fail is different from being reckless, and being careful doesn\u2019t mean avoiding risk. Yet, too often, we treat risk as a binary choice: go all in or play it safe.</p> <p>The truth? Smart risk-taking lies in the middle.</p> <p>Balance is key \u2026 too cautious, and you miss opportunities; too reckless, and you invite disaster.</p> <p>Across industries, progress comes from calculated risks:</p> <ul> <li>Entrepreneurs invest in new ventures, but only after research and planning.</li> <li>Engineers make changes that can break systems, but only after testing.</li> <li>Scientists expect failure, but use it to fuel discoveries.</li> </ul> <p>So how do we take risks wisely?</p> <ul> <li>Redefine failure: it\u2019s not the end; it\u2019s more data to try again.</li> <li>Encourage experiments: a proof of concept prevents analysis paralysis.</li> <li>Think critically: some risks aren\u2019t worth taking, but avoiding them entirely is its own risk.</li> <li>Build resilience: mistakes are embarrassing, but always playing it safe is worse.</li> </ul> <p>Why does this matter in software?</p> <p>Move too fast, and you break production. The systems we support are often mission-critical. Outages cost, time, money, and trust \u2026 not just lost productivity, but in missed opportunities to serve customers.</p> <p>Careful engineering doesn\u2019t mean standing still. Systems must evolve \u2026 updating platforms, managing tech debt, and delivering new functionality to remain valuable.</p> <p>The key? Take smart, calculated risks. Experiment. Learn. Adapt. That\u2019s how we build software \u2014 and careers \u2014 that thrive.</p>"},{"location":"blog/2024/02/18/robotstxt/","title":"robots.txt","text":""},{"location":"blog/2024/02/18/robotstxt/#care-and-feeding-of-ai","title":"Care and Feeding of AI","text":"<p>If you would rather not feed OpenAI or Google AI models after midnight \u2026 </p> <p>Here is a good summary of how to use a robots.txt file to limit what OpenAI and Google will ingest from your website for training data.</p> <p>Kudos to the authors for finding a valid use of the word gauche.</p> <p>Read more here:</p> <p>www.eff.org</p>"},{"location":"blog/2023/08/28/security-defaults/","title":"Security Defaults","text":"<p>Security access controls extend into your databases. The principle of least privilege needs to be enforced not only for who can connect but also for what they can do within your databases.</p> <p>For example, until PostgreSQL version 15, PUBLIC (which all users are a member of) could create tables within the public schema unless REVOKE\u2019d. This is just one example.</p> <p>It\u2019s important to review what the security defaults are for your database product to ensure you are enforcing the least privilege access model where you explicitly grant access to resources.</p> <p>PostgreSQL 15.0 Release Notes</p>"},{"location":"blog/2023/07/24/security-through-obscurity/","title":"Security Through Obscurity","text":"<p>Another example of why security through obscurity does not work.  Radio systems used by police and military outside the US have vulnerabilities that have existed since the 1990s.</p> <p>The system is also used within \u201cpipelines, railways, the electric grid, mass transit, and freight trains\u201d in many countries, including the U.S.</p> <p>Do you agree that security through obscurity is not effective?</p> <p>Code Kept Secret for Years Reveals Its Flaw\u2014a Backdoor</p>"},{"location":"blog/2023/07/16/what-is-a-securitytxt-file/","title":"What is a security.txt file?","text":"<p>A security.txt file on your website helps security researchers report security vulnerabilities.</p> <ul> <li>Full disclosure \u2026 I hadn\u2019t run across security.txt as being \u201ca thing\u201d before this week.</li> <li>A security.txt file on your site lets you provide contact information for how you want to receive reports from security researchers. </li> <li>Consider if you would like to provide an email or a web form.  A secure web form is probably a better choice.</li> <li>Brian Krebs has reported that spam \u201csecurity reports\u201d go up with a security.txt file.</li> </ul> <p>I am not convinced everyone needs to have a security.txt file in place.  If you have only a static web page, the cost of using email and wading through spam reports would be higher than any benefits.</p> <p>I do think companies with bug bounty programs should have one in place to direct researchers to their bug bounty site.</p> <p>Site where I originally learned about security.txt files:</p> <ul> <li>Advocating security.txt across UK government</li> </ul> <p>Site to help you create a security.txt file:</p> <ul> <li>securitytxt.org</li> </ul> <p>Brian Krebs\u2019 article on security.txt files:</p> <ul> <li>Does Your Organization Have a Security.txt File?</li> </ul>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/","title":"Old Macbook Pro + Pop!_OS","text":"<p>This weekend I took my old Macbook Pro from 2013 and installed Pop!_OS. It was easy, and so far I've been pleased.</p> <p>This guide will walk you through setting up Pop!_OS on a Mac 11,1, including configuring Wi-Fi. There are also some notes on installing essential tools.</p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#setting-up-pop_os-on-a-mac-111","title":"Setting Up Pop!_OS on a Mac 11,1","text":"<p>I started my research with the following blog post by Alexander Swensen: Installing Pop!_OS (Linux) on a Late 2011 MacBook Pro. </p> <p>TLDR; The installation involves downloading Pop!_OS and flashing it onto a USB drive using balenaEtcher. Then you power on holding the option key so you can boot from the USB drive.</p> <p>It will take awhile to boot depending on the speed of your USB drive and Mac; so you can go get coffee. Everything after that is a straightforward install except one thing: your Wi-Fi driver. </p> <p>Luckily I had an adaptor for USB to RJ45 to let me get a wired connection. </p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#enable-vi-command-line-editing-for-bash","title":"Enable vi Command Line Editing for Bash","text":"<p>Of course, the first thing to do is to open a shell, and update your command line editor to be vi. I guess this is optional, but I get confused if I don't have my normal command line editor set up.</p> <p>Open your <code>.bashrc</code> file:</p> <pre><code>vi ~/.bashrc\n</code></pre> <p>Add the following at the end of the file:</p> <pre><code># Enable vi command line editing\nset -o vi\n</code></pre> <p>Note: Terminal cut and paste uses <code>Ctrl+Shift+C</code> and <code>Ctrl+Shift+V</code>.</p> <p>Then you can either <code>source ~/.bashrc</code> or just reopen your terminal.</p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#macbook-pro-111-late-2013-wi-fi-networking","title":"Macbook Pro 11,1 (Late 2013) Wi-Fi Networking","text":"<p>Setting up networking for a 2013 Macbook Pro is relatively straightforward. </p> <p>Use a wired adapter for the initial setup and go to Settings &gt; Software &amp; Updates &gt; Pop!_OS Software and enable:</p> <ul> <li>Canonical-supported free and open-source software  </li> <li>Proprietary drivers for devices (restricted)</li> </ul> <p>Then open a terminal to check your hardware:</p> <pre><code>lspci -nn -d 14e4:\n</code></pre> <p>Look for Broadcom (14e4) to confirm your network adapter type. Example output:</p> <pre><code>02:00.0 Multimedia controller [0480]: Broadcom Inc. and subsidiaries 720p FaceTime HD Camera [14e4:1570]\n03:00.0 Network controller [0280]: Broadcom Inc. and subsidiaries BCM4360 802.11ac Dual Band Wireless Network Adapter [14e4:43a0] (rev 03)\n</code></pre> <p>If you see Broadcom, great!</p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#get-current","title":"Get Current","text":"<p>These steps might not be required, but starting from a known, up-to-date state is simpler to debug.</p> <ul> <li>Update your package list:</li> </ul> <pre><code>sudo apt update\n</code></pre> <ul> <li>Update your PCI IDs list:</li> </ul> <pre><code>sudo update-pciids\n</code></pre>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#broadcom-driver","title":"Broadcom Driver","text":"<ul> <li>Install the Broadcom drivers:</li> </ul> <pre><code>sudo apt install --reinstall broadcom-sta-dkms\n</code></pre>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#sugestions-for-basic-setup","title":"Sugestions for Basic Setup","text":"<p>These are purely optional, and they are just suggestions. My current M3 Macbook Pro is my daily driver, and this older laptop will just be a laptop that is in our kitchen to be used mostly for web browsing.</p> <p>Still, I like to have a few things on all computers. I already switch between Windows and Mac (and now Linux) for desktops.  So consistency is important.</p> <p>I prefer to keep my chosen shell, editor, and email consistent across machines. I always set up git too even on systems not used for development.</p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#install-zsh","title":"Install zsh","text":"<ul> <li>Install zsh:</li> </ul> <pre><code>sudo apt install zsh\n</code></pre> <ul> <li>Install Oh My Zsh:</li> </ul> <pre><code>sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\n</code></pre> <p>When prompted, choose to set zsh as your default shell.</p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#add-visual-studio-code","title":"Add Visual Studio Code","text":"<ul> <li>While the app store can be used to install the Flatpak for VS Code, it's recommended to install it directly from Microsoft for easier updates: visualstudio.com.</li> <li>Download the <code>.deb</code> package.</li> <li>Open a terminal and run the following (replace <code>&lt;package-name&gt;</code> with the actual file name):</li> </ul> <pre><code>cd ~/Downloads\nsudo apt install ./&lt;package-name&gt;.deb\n</code></pre>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#add-shell-information-in-vs-code","title":"Add Shell Information in VS Code","text":"<ul> <li> <p>Open VS Code settings:</p> </li> <li> <p>Press <code>Ctrl+Shift+P</code> and search for <code>Preferences: Open Settings (JSON)</code>.</p> </li> <li> <p>Add the following to your settings:</p> </li> </ul> <pre><code>{\n  \"terminal.integrated.defaultProfile.linux\": \"zsh\",\n  \"terminal.integrated.profiles.linux\": {\n    \"zsh\": {\n      \"path\": \"/usr/bin/zsh\"\n    },\n    \"bash\": {\n      \"path\": \"/usr/bin/bash\"\n    }\n  }\n}\n</code></pre>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#set-vs-code-as-the-default-editor","title":"Set VS Code as the Default Editor","text":"<ul> <li>In the terminal, run:</li> </ul> <pre><code>sudo update-alternatives --set editor /usr/bin/code\n</code></pre>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#add-prettier-extension-to-vs-code","title":"Add Prettier Extension to VS Code","text":"<ul> <li>Install the Prettier extension and add the following to your user settings:</li> </ul> <pre><code>{\n  \"editor.defaultFormatter\": \"esbenp.prettier-vscode\",\n  \"[javascript]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  }\n}\n</code></pre>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#check-for-software-updates","title":"Check for Software Updates","text":"<ul> <li>Open Pop!_Shop.</li> <li>Press <code>Ctrl+I</code> to view and apply updates.</li> </ul>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#outlookcom","title":"Outlook.com","text":"<p>I use Outlook.com and like to have email available wherever I\u2019m logged in. While you can add your Microsoft account and use Geary (the default app) for email, I have found the Outlook as a PWA (Progressive Web App) to have a better UI.</p> <p>To install Edge, download the <code>.deb</code> package from the Microsoft Edge website and install it with:</p> <pre><code>sudo apt install ./&lt;package-name&gt;.deb\n</code></pre> <p>After logging in to Outlook.com in Edge, you can choose to install it as a PWA for a more app-like experience.</p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#lastpass","title":"LastPass","text":"<p>My preferred password manager is LastPass. There are LastPass extensions for all major browsers, including Edge and Firefox.</p> <p>You should have no trouble installing and enabling the extension in your browser of choice to login to an existing account. </p> <p>If setting up new, make sure to use a strong master password and enable multi-factor authentication for added security.</p>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#github-cli","title":"GitHub CLI","text":"<p>You can install the GitHub CLI following GitHub\u2019s official documentation. This tool makes it easy to log in, manage repositories, and automate adding an SSH key to your new Linux install.</p> <p>After installing, don\u2019t forget to set your Git user name and email:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>"},{"location":"blog/2025/06/21/old-macbook-pro--pop_os/#final-thoughts","title":"Final Thoughts","text":"<p>I was pleasantly surprised by the ease of installation and the level of support for my old MacBook Pro. Installing the Wi-Fi driver was the only step that required customization beyond a standard install.</p> <p>As an early adopter of Linux in the '90s, I had a love/hate relationship with desktop Linux over the years. When I bought my first MacBook in 2013, I stopped using Linux outside of server environments.</p> <p>Pop!_OS has been excellent so far. It\u2019s based on Ubuntu, so you should find it familiar and easy to use. The hardware support, especially for older Macs, is impressive. If you\u2019re looking to breathe new life into an old MacBook, I highly recommend giving Pop!_OS a try.</p> <p>If you have any questions or run into issues, the Pop!_OS and Ubuntu communities are active and helpful. Happy computing!</p>"},{"location":"blog/2024/05/02/stored-procedures/","title":"Stored Procedures","text":"<p>One valid use case for stored procedures in operational systems can be to reduce round trips to the database for some transactions. In (some) databases, it can also mean running compiled code close to the database. </p> <p>Both are more efficient in some cases, but it can also be more difficult to debug stored procedures than normal code.</p> <p>Furthermore, after the person who wrote them moves on, stored procedures are often a \u201chidden\u201d part of your code base. Not all developers will have experience with stored procedures.</p> <ul> <li> <p>Do you find performance a compelling argument for stored procedures (in some cases)? </p> </li> <li> <p>Do you agree stored procedures are frequently \u201chidden code\u201d less understood by the average developer?</p> </li> </ul>"},{"location":"blog/2023/08/08/trading-texts/","title":"Trading Texts","text":"<p>This article on fines for major banks whose employees used \u201cpersonal messaging apps to discuss deals, trades and other business\u201d is a good reminder to know and follow the laws and regulations for your industry.</p> <p>Now if we can get folks to stop texting and driving too!</p> <p>US regulators fine Wall Street firms $549 mln in latest texting probe</p>"},{"location":"blog/2025/03/21/distributed-transactions/","title":"Distributed Transactions","text":"<p>Most engineers think they understand transactions until they deal with distributed systems, different storage engines, or frameworks that abstract too much. </p>"},{"location":"blog/2025/03/21/distributed-transactions/#common-pitfalls","title":"Common Pitfalls","text":"<ul> <li>Rollback means everything is undone: Not for external calls or side effects.</li> <li>It\u2019s all or nothing: Only if a single system is involved. Across databases and queues, you need two-phase commit (2PC) or compensating transactions and/or robust retry.</li> <li>2PC is better: 2PC means simpler code but requires full protocol support from all participants and has performance implications.</li> <li>Compensating transactions are better: While they reduce coupling, they also require more error handling and introduce eventual consistency.</li> <li>Nested transactions are simple: Nope. They cause unexpected locking and failure behavior. In longer lived code bases they often surprise engineers who do not know they are there.</li> </ul>"},{"location":"blog/2025/03/21/distributed-transactions/#what-engineers-should-focus-on","title":"What Engineers Should Focus On","text":"<ul> <li>Understand what the behavior is for your storage engine for transactions, isolation, rollback, message delivery, and retry.</li> <li>Test error handling for all failure scenarios, including partial failures and retries.</li> <li>Handle side effects explicitly with patterns like the outbox pattern and idempotent operations.</li> <li>Avoid external calls in transactions if possible. These can make transactions run longer and have external side effects.</li> <li>Keep transactions fast and small to reduce database contention and locking issues.</li> <li>Pick a strategy that works for YOUR system based on trade-offs. Don\u2019t make it a \u201creligious\u201d decision. There are environments where 2PC will work well, and eventual consistency is OK for some use cases.</li> </ul>"},{"location":"blog/2025/03/21/distributed-transactions/#bottom-line","title":"Bottom Line","text":"<p>Transactions aren\u2019t magic. If you don\u2019t deeply understand how they work in your system, assumptions will break under load and failure. </p> <p>Choose what is best for your system, whether compensating transactions or 2PC, and test for failure conditions.</p>"},{"location":"blog/2023/06/09/unexpected-behavior/","title":"Unexpected Behavior","text":"<p>\u201cSeek simplicity and distrust it.\u201d ~ Alfred Whitehead</p> <p>As an example of something simple that you might want to distrust, let us look at default error handling in <code>bash</code>.  </p> <p>You can cut &amp; paste the short scripts below into bash to follow along.</p>"},{"location":"blog/2023/06/09/unexpected-behavior/#failure-is-an-option","title":"Failure Is An Option","text":"<p>Script: <pre><code>false\necho $?\n</code></pre> Result: <pre><code>1\n</code></pre> This is simple enough.  The <code>false</code> command sets a return code of 1.  The <code>echo $?</code> displays the return code for the last command.</p>"},{"location":"blog/2023/06/09/unexpected-behavior/#script-return-codes","title":"Script Return Codes","text":"<p>If I put the same code in a bash script, you get the same result.</p> <p>Script: <pre><code>cat &gt; notok.sh &lt;&lt; SCRIPTEND\n#!/bin/bash\nfalse\nSCRIPTEND\nchmod +x notok.sh\n./notok.sh\necho $?\n</code></pre> Result: <pre><code>1\n</code></pre></p>"},{"location":"blog/2023/06/09/unexpected-behavior/#here-documents","title":"Here Documents","text":"<p>The example above uses a \"Here Document\" (<code>&lt;&lt; SCRIPTEND</code> to <code>SCRIPTEND</code>).  \"Here Documents\" are a topic for another blog, but they allow you to label a section of inline text to be passed to other parts of a script. </p> <p>In this case, the script between the labels is passed to the <code>cat &gt; notok.sh</code> file redirection statement to create a script.</p> <p>To see what is written to the notok.sh script, just <code>cat</code> the file created.</p> <p>Command: <pre><code>cat notok.sh\n</code></pre></p> <p>Output: <pre><code>#!/bin/bash\nfalse\n</code></pre></p>"},{"location":"blog/2023/06/09/unexpected-behavior/#multiple-return-codes","title":"Multiple Return Codes","text":"<p>Now that we have a basic script to start with \u2026 what happens if I add another command that does not set a return code of 1?  </p> <p>In this case, there is a return code of 1 (<code>false</code>) followed by a return code of 0 (<code>true</code>) in the script.</p> <p>Script: <pre><code>cat &gt; notok.sh &lt;&lt; SCRIPTEND\n#!/bin/bash\nfalse\ntrue\nSCRIPTEND\nchmod +x notok.sh\n./notok.sh\necho $?\n</code></pre> Result: <pre><code>0\n</code></pre></p>"},{"location":"blog/2023/06/09/unexpected-behavior/#last-return-code-wins","title":"Last Return Code Wins","text":"<p>We can learn two things: 1) the script is returning the return code from the <code>true</code> command 2) the script is not exiting on non-zero return codes.</p> <p>While we could argue whether this is a sensible behavior, it is the expected behavior for bash. </p> <p>Personally, I prefer scripts to exit on unexpected return codes.  </p>"},{"location":"blog/2023/06/09/unexpected-behavior/#its-a-trap","title":"It's a Trap!","text":"<p>One way to make a script exit on a non-zero return code is to add an error trap.</p> <p>Note the <code>\\</code> characters below are only there to keep the variables in the here document from being expanded before being written to the script file.</p> <p>Script: <pre><code>cat &gt; notok.sh &lt;&lt; SCRIPTEND\n#!/bin/bash\n\ntrap 'on_err \\$? \\$LINENO' ERR\n\non_err() {\n  echo \"Error: Unexpected return code \\$1 on line \\$2\"\n  exit 1;\n}\n\nfalse\ntrue\nSCRIPTEND\nchmod +x notok.sh\n./notok.sh\necho $?\n</code></pre> Result: <pre><code>Error: Unexpected return code 1 on line 10\n1\n</code></pre></p> <p>The <code>trap 'on_err $? $LINENO' ERR</code> statement does the following: </p> <ul> <li>Catches errors with <code>ERR</code> keyword</li> <li>Calls the <code>on_err()</code> function passing the return code <code>$?</code> and <code>$LINENO</code></li> <li>The function <code>on_err()</code> then shows the return code and line number information before exiting with a return code of 1</li> </ul> <p>You can see from the <code>cat -n</code> command below it is finding the correct line number via the <code>$LINENO</code> variable.  This shell variable is managed by bash.</p> <p>Command: <pre><code>cat -n notok.sh\n</code></pre></p> <p>Output: <pre><code>cat -n notok.sh\n 1  #!/bin/bash\n 2  \n 3  trap 'on_err $? $LINENO' ERR\n 4  \n 5  on_err() {\n 6    echo \"Error: Unexpected return code $1 on line $2\"\n 7    exit 1;\n 8  }\n 9  \n10  false\n11  true\n</code></pre></p> <p>The function name does not have to be on_err, and it could also perform cleanup before exiting.  One example could be cleaning up files created by <code>mktemp</code> if that was used earlier in the script.</p> <p>But what happens if I put the false statement followed by a true statement inside a function?  Depending on your background, you might expect it to still trap the error, but you would be wrong.</p> <p><pre><code>cat &gt; notok.sh &lt;&lt; SCRIPTEND\n#!/bin/bash\n\ntrap 'on_err \\$? \\$LINENO' ERR\n\non_err() {\n  echo \"Error: Unexpected return code \\$1 on line \\$2\"\n  exit 1;\n}\n\nrun_it() {\n  false\n  true\n}\nrun_it\nSCRIPTEND\nchmod +x notok.sh\n./notok.sh\necho $?\n</code></pre> Result: <pre><code>0\n</code></pre></p> <p>Why the difference in behavior?  By default, in bash a function returns the last return code, and that is the return code which would be trapped (if any).</p> <p>You can turn on an option, however, to tell the script to trap return codes in functions: <code>set -E</code> </p> <p><pre><code>cat &gt; notok.sh &lt;&lt; SCRIPTEND\n#!/bin/bash\nset -E\n\ntrap 'on_err \\$? \\$LINENO' ERR\n\non_err() {\n  echo \"Error: Unexpected return code \\$1 on line \\$2\"\n  exit 1;\n}\n\nrun_it() {\n  false\n  true\n}\nrun_it\nSCRIPTEND\nchmod +x notok.sh\n./notok.sh\necho $?\n</code></pre> Result: <pre><code>Error: Unexpected return code 1 on line 11\n1\n</code></pre></p> <p>Are we there yet?  Well it depends on if you care about unexpected return codes within pipelines.  </p> <p>Consider the following:</p> <p><pre><code>cat &gt; notok.sh &lt;&lt; SCRIPTEND\n#!/bin/bash\nset -E\n\ntrap 'on_err \\$? \\$LINENO' ERR\n\non_err() {\n  echo \"Error: Unexpected return code \\$1 on line \\$2\"\n  exit 1;\n}\n\nrun_it() {\n  false | tee\n  true\n}\nrun_it\nSCRIPTEND\nchmod +x notok.sh\n./notok.sh\necho $?\n</code></pre> Result: <pre><code>0\n</code></pre> Adding <code>| tee</code> above means the pipeline returns a return code of 0 instead of the 1 that is set by false.  If we want to trap these errors too, we can add <code>set -o pipefail</code>.</p> <p>Whether you want to trap pipeline errors depends on what you want your scripts to do.</p> <p>If you turn on <code>pipefail</code> you might be surprised by grep when it does not match a search string.  In that case, grep sets <code>$?</code> equal to 1.</p> <p><pre><code>cat &gt; notok.sh &lt;&lt; SCRIPTEND\n#!/bin/bash\nset -E\nset -o pipefail\n\ntrap 'on_err \\$? \\$LINENO' ERR\n\non_err() {\n  echo \"Error: Unexpected return code \\$1 on line \\$2\"\n  exit 1;\n}\n\nrun_it() {\n  true | grep \"epic fail\"\n}\nrun_it\nSCRIPTEND\nchmod +x notok.sh\n./notok.sh\necho $?\n</code></pre> Result: <pre><code>Error: Unexpected return code 1 on line 12\n1\n</code></pre></p>"},{"location":"blog/2023/06/09/unexpected-behavior/#in-summary","title":"In Summary","text":"<pre><code>#!/bin/bash\nset -E\nset -o pipefail\n\ntrap 'on_err $? $LINENO' ERR\n\non_err() {\n  echo \"Error: Unexpected return code $1 on line $2\"\n  exit 1;\n}\n</code></pre> <p>You can set a trap to catch errors in your scripts that use functions and pipelines by adding the above code.</p> <p>Happy coding!</p>"},{"location":"blog/2025/04/02/human-interaction/","title":"Human Interaction","text":"<p>Here is an interesting article that reminds us to consider human interaction when designing technology.</p> <p>The article is a summary of a research paper where delivery robots were followed while their interactions were recorded. </p> <p>TLDR; the researchers found people will modify their behavior to help robots along.</p> <p>\ud83e\udd14 While the article is specific to human-robot-environment interaction, I think it\u2019s a fun reminder to review usability and accessibility while designing new systems.</p> <p>New study finds invisible 'human work' allows robots to make deliveries</p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/","title":"Team Dynamics on Vacation","text":"<p>\u201cI can do things you cannot, you can do things I cannot; together we can do great things.\u201d ~ Mother Teresa</p> <p>Did you take your team on vacation with you?  Did they go with you on your hike?  Were they floating on the lazy river next to you?  No?</p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/#time-off","title":"Time Off","text":"<p>If your team was negatively affected by your absence, you might be responsible for letting your team down.</p> <p>Don\u2019t take this the wrong way.  </p> <p>You should be able to leave work for a while without work piling up like garbage during a sanitation strike!</p> <p>You will, however, need to make sure your team is prepared for your absence.</p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/#high-performing-teams","title":"High Performing Teams","text":"<p>High performing teams ensure no one is irreplaceable, cross-training each other as opportunities present themselves.  Or they create the opportunity by setting aside time.</p> <p>When someone is on an extended absence, teammates step-up and handle tasks that cannot wait.</p> <p>For important, time-sensitive tasks to get done while you are gone \u2026 your teammates will need to have the capability, confidence, and capacity to complete the tasks.</p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/#capability","title":"Capability","text":"<p>Do you need to cross-train your teammates?</p> <p>Some tasks may be self-explanatory to you, but are they for others?  The only way to find out is to work together to find out.</p> <p>If someone is \u201chit by a bus\u201d, your team should be able to keep moving forward.</p> <p>Make sure everyone has a backup.</p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/#confidence","title":"Confidence","text":"<p>Do your teammates have confidence they can complete a task?  </p> <p>Confidence normally means having done a task before, or at least being familiar with all the aspects of a task.  </p> <p>Practice tasks together. Let the person learning \u201cdrive\u201d.</p> <p>Or it\u2019s possible a teammate is capable but literally lacks confidence.  If this is the case, tell them you have confidence in them.  People do not read minds.</p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/#capacity","title":"Capacity","text":"<p>Perhaps your team is over worked?</p> <p>Team depth must include capacity to absorb workload changes. </p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/#automation","title":"Automation","text":"<p>Staffing may not be something you can change, but can you automate anything?  </p> <p>Tasks that are automated won\u2019t go waiting while you are gone.</p>"},{"location":"blog/2023/07/30/team-dynamics-on-vacation/#please-and-thank-you","title":"Please and Thank-you","text":"<p>Do teammates know they have your approval to help?  </p> <p>Politely ask for help while you are out. Remember, say please and thank-you.</p> <p>Happy Vacation!</p>"},{"location":"blog/2024/02/18/voyager/","title":"Voyager","text":""},{"location":"blog/2024/02/18/voyager/#long-distance-relationship","title":"Long-Distance Relationship","text":"<p>Voyager 1 and Voyager 2 were launched in 1977 and have been traveling farther from home each year.</p> <p>Recently, work has been undertaken to update their software to correct bugs in the attitude articulation and control system (AACS) software for both.</p> <p>From PC Mag:</p> <p>\u201cAs Voyager 2 is over 12 billion miles away, it took over 18 hours to send the software patch to the probe \u2026\u201d </p> <p>Read more on PC Mag\u2019s site: www.pcmag.com</p>"},{"location":"blog/2023/08/19/wayback-machine/","title":"WayBack Machine","text":"<p>\"I don\u2019t have a photograph, but you can have my footprints. They\u2019re upstairs in my socks.\" ~ Groucho Marx </p>"},{"location":"blog/2023/08/19/wayback-machine/#an-unexpected-search","title":"An Unexpected Search","text":"<p>Several recent events have had me thinking more about open-source.  The same as many people in the 1990s, my introduction to open-source was with Linux.  I remember installing Slackware Linux and later switching to Red Hat Linux.</p> <p>My wife would probably tell you I talked about Linux and open-source too much back then.  I remember finding the topic exciting and wanting to learn more.  Beyond that, what my specific thoughts were on the controversies of the day, I can't remember.  </p> <p>So I wanted to find a college paper I wrote on Linux on the WayBack Machine to compare with my current thoughts.  </p> <p>What I found surprised me.  </p>"},{"location":"blog/2023/08/19/wayback-machine/#party-like-its-1999","title":"Party Like It's 1999","text":"<p>Before I could use the WayBack Machine, I had to remember the home page for the site where I posted my paper.  After much scrunching of my face, I finally remembered my personal site was on mama.instate.edu.  </p> <p>I didn't expect to find my paper there because I had requested my account be closed a few years after I graduated, but I wanted to see if it still existed.  </p> <p>Most of the links no longer work, but the site is still running and looks like it did in 1999!  </p> <p></p>"},{"location":"blog/2023/08/19/wayback-machine/#searching-for-answers","title":"Searching for Answers","text":"<p>Next, I headed over to the WayBack Machine to try to find my college paper.  Sadly, my old site had only been captured after I took the paper down, but there was a search engine form on my site.  </p> <p>On a lark, I tried Yahoo just to see if it still worked.  While I knew that the Internet Archive changes links to keep you within the WayBack machine, I didn't expect the search to work based on keywords.  </p> <p>For common terms, it's really cool that you can search Yahoo in the past with the WayBack Machine.</p> <p>Try the link below to check this out.  Just search for a term that is common (baseball, socks, puppies)</p> <p>Yahoo Circa 2000</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/linux/","title":"Linux","text":""},{"location":"blog/category/programming/","title":"Programming","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/history/","title":"History","text":""},{"location":"blog/category/leadership/","title":"Leadership","text":""},{"location":"blog/category/design/","title":"Design","text":""},{"location":"blog/category/modernization/","title":"Modernization","text":""},{"location":"blog/category/culture/","title":"Culture","text":""},{"location":"blog/category/knowledge-transfer/","title":"Knowledge Transfer","text":""},{"location":"blog/category/retirement/","title":"Retirement","text":""},{"location":"blog/category/research/","title":"Research","text":""},{"location":"blog/category/risk/","title":"Risk","text":""},{"location":"blog/category/usability/","title":"Usability","text":""},{"location":"blog/category/database/","title":"Database","text":""},{"location":"blog/category/simplicity/","title":"Simplicity","text":""},{"location":"blog/category/humor/","title":"Humor","text":""},{"location":"blog/category/optionality/","title":"Optionality","text":""},{"location":"blog/category/learning/","title":"Learning","text":""},{"location":"blog/category/legislation/","title":"Legislation","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/teams/","title":"Teams","text":""},{"location":"blog/category/standards/","title":"Standards","text":""},{"location":"blog/category/coding/","title":"Coding","text":""},{"location":"blog/category/automation/","title":"Automation","text":""},{"location":"blog/page/2/","title":"Index","text":""},{"location":"blog/page/3/","title":"Index","text":""},{"location":"blog/page/4/","title":"Index","text":""},{"location":"blog/page/5/","title":"Index","text":""},{"location":"blog/archive/2025/page/2/","title":"2025","text":""},{"location":"blog/archive/2025/page/3/","title":"2025","text":""},{"location":"blog/archive/2023/page/2/","title":"2023","text":""}]}